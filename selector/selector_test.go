package selector

import (
	"log/slog"
	"testing"

	"go.ntppool.org/monitor/ntpdb"
)

// TestZeroActiveMonitorsWithConstraintViolations tests that testing monitors with
// constraint violations can be demoted when there are zero active monitors
func TestZeroActiveMonitorsWithConstraintViolations(t *testing.T) {
	sl := &Selector{
		log: testLogger(),
	}

	// Create test data: 0 active, 4 testing monitors with constraint violations
	evaluatedMonitors := []evaluatedMonitor{
		{
			monitor: monitorCandidate{
				ID:           1,
				AccountID:    ptr(uint32(1)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
			},
			currentViolation: &constraintViolation{
				Type:    violationLimit,
				Details: "account limit exceeded",
			},
			recommendedState: candidateOut,
		},
		{
			monitor: monitorCandidate{
				ID:           2,
				AccountID:    ptr(uint32(1)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
			},
			currentViolation: &constraintViolation{
				Type:    violationLimit,
				Details: "account limit exceeded",
			},
			recommendedState: candidateOut,
		},
		{
			monitor: monitorCandidate{
				ID:           3,
				AccountID:    ptr(uint32(2)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
			},
			currentViolation: &constraintViolation{
				Type:    violationNetworkDiversity,
				Details: "network diversity violation",
			},
			recommendedState: candidateOut,
		},
		{
			monitor: monitorCandidate{
				ID:           4,
				AccountID:    ptr(uint32(2)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
			},
			currentViolation: &constraintViolation{
				Type:    violationNetworkDiversity,
				Details: "network diversity violation",
			},
			recommendedState: candidateOut,
		},
	}

	server := &serverInfo{
		ID:        14,
		AccountID: ptr(uint32(3)),
	}

	accountLimits := map[uint32]*accountLimit{
		1: {AccountID: 1, MaxPerServer: 2, ActiveCount: 0, TestingCount: 2},
		2: {AccountID: 2, MaxPerServer: 2, ActiveCount: 0, TestingCount: 2},
	}

	changes := sl.applySelectionRules(t.Context(), evaluatedMonitors, server, accountLimits, nil)

	// Debug output
	for _, change := range changes {
		t.Logf("Change: monitor %d from %s to %s, reason: %s",
			change.monitorID, change.fromStatus, change.toStatus, change.reason)
	}

	// With zero active monitors, we should allow demotions
	demotionCount := 0
	promotionCount := 0
	for _, change := range changes {
		if change.fromStatus == ntpdb.ServerScoresStatusTesting &&
			change.toStatus == ntpdb.ServerScoresStatusCandidate {
			demotionCount++
		}
		if change.toStatus == ntpdb.ServerScoresStatusActive {
			promotionCount++
		}
	}

	// Should demote at least some testing monitors with violations
	if demotionCount == 0 {
		t.Errorf("Expected demotions when zero active monitors, got %d", demotionCount)
	}

	// With current logic, emergency override will promote if there are zero active,
	// but since we have constraint violations and candidateOut, the current implementation
	// might not promote them. Let's check what actually happened.
	if promotionCount == 0 && demotionCount == 4 {
		// This is actually correct behavior - demote all violating monitors first
		t.Logf("Correct behavior: demoted all constraint-violating monitors")
	} else if promotionCount > 0 {
		t.Logf("Emergency override kicked in and promoted %d monitors", promotionCount)
	}

	t.Logf("Changes: %d demotions, %d promotions out of %d total changes",
		demotionCount, promotionCount, len(changes))
}

// TestIterativeAccountLimitEnforcement tests that account limits are properly
// enforced when promoting monitors iteratively
func TestIterativeAccountLimitEnforcement(t *testing.T) {
	sl := &Selector{
		log: testLogger(),
	}

	// Create test data: 3 active (to prevent emergency override),
	// 3 testing monitors from same account, account limit is 2
	evaluatedMonitors := []evaluatedMonitor{
		// Active monitors to prevent emergency override
		{
			monitor: monitorCandidate{
				ID:           10,
				AccountID:    ptr(uint32(2)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusActive,
				IsHealthy:    true,
			},
			recommendedState: candidateIn,
		},
		{
			monitor: monitorCandidate{
				ID:           11,
				AccountID:    ptr(uint32(2)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusActive,
				IsHealthy:    true,
			},
			recommendedState: candidateIn,
		},
		{
			monitor: monitorCandidate{
				ID:           12,
				AccountID:    ptr(uint32(3)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusActive,
				IsHealthy:    true,
			},
			recommendedState: candidateIn,
		},
		{
			monitor: monitorCandidate{
				ID:           1,
				AccountID:    ptr(uint32(1)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
				Count:        int64(minCountForActive), // Ensure testing monitors can be promoted to active
			},
			recommendedState: candidateIn,
		},
		{
			monitor: monitorCandidate{
				ID:           2,
				AccountID:    ptr(uint32(1)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
				Count:        int64(minCountForActive), // Ensure testing monitors can be promoted to active
			},
			recommendedState: candidateIn,
		},
		{
			monitor: monitorCandidate{
				ID:           3,
				AccountID:    ptr(uint32(1)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
				Count:        int64(minCountForActive), // Ensure testing monitors can be promoted to active
			},
			recommendedState: candidateIn,
		},
	}

	server := &serverInfo{
		ID:        1,
		AccountID: ptr(uint32(2)), // Different account
	}

	// Account limit: max 2 active per server
	accountLimits := map[uint32]*accountLimit{
		1: {AccountID: 1, MaxPerServer: 2, ActiveCount: 0, TestingCount: 3},
		2: {AccountID: 2, MaxPerServer: 2, ActiveCount: 2, TestingCount: 0},
		3: {AccountID: 3, MaxPerServer: 2, ActiveCount: 1, TestingCount: 0},
	}

	changes := sl.applySelectionRules(t.Context(), evaluatedMonitors, server, accountLimits, nil)

	// Debug output
	for _, change := range changes {
		t.Logf("Change: monitor %d from %s to %s, reason: %s",
			change.monitorID, change.fromStatus, change.toStatus, change.reason)
	}

	// Count promotions to active
	promotionCount := 0
	for _, change := range changes {
		if change.fromStatus == ntpdb.ServerScoresStatusTesting &&
			change.toStatus == ntpdb.ServerScoresStatusActive {
			promotionCount++
		}
	}

	// With per-status change limits (promotions=2 with 3 active monitors),
	// should promote up to 2 monitors per cycle (account limit also allows 2)
	if promotionCount != 2 {
		t.Errorf("Expected 2 promotions (promotion limit), got %d", promotionCount)
		t.Logf("Note: Both account limit and promotion limit allow 2")
	}
}

// TestEmergencyOverrideBehavior tests that emergency override works correctly
// when there are zero active monitors
func TestEmergencyOverrideBehavior(t *testing.T) {
	sl := &Selector{
		log: testLogger(),
	}

	// Create test data: 0 active, testing monitors with violations
	evaluatedMonitors := []evaluatedMonitor{
		{
			monitor: monitorCandidate{
				ID:           1,
				AccountID:    ptr(uint32(1)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
			},
			currentViolation: &constraintViolation{
				Type: violationLimit,
			},
			recommendedState: candidateOut,
		},
		{
			monitor: monitorCandidate{
				ID:           2,
				AccountID:    ptr(uint32(2)),
				GlobalStatus: ntpdb.MonitorsStatusActive,
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				HasMetrics:   true,
				IsHealthy:    false, // Unhealthy
			},
			recommendedState: candidateOut,
		},
		{
			monitor: monitorCandidate{
				ID:           3,
				AccountID:    ptr(uint32(3)),
				GlobalStatus: ntpdb.MonitorsStatusActive, // Must be globally active for emergency promotion
				ServerStatus: ntpdb.ServerScoresStatusTesting,
				IsHealthy:    true,
				Count:        int64(minCountForActive), // Ensure testing monitors can be promoted to active
			},
			recommendedState: candidateIn,
		},
	}

	server := &serverInfo{
		ID:        1,
		AccountID: ptr(uint32(4)),
	}

	accountLimits := map[uint32]*accountLimit{
		1: {AccountID: 1, MaxPerServer: 1, ActiveCount: 0, TestingCount: 1},
		2: {AccountID: 2, MaxPerServer: 1, ActiveCount: 0, TestingCount: 1},
		3: {AccountID: 3, MaxPerServer: 1, ActiveCount: 0, TestingCount: 1},
	}

	changes := sl.applySelectionRules(t.Context(), evaluatedMonitors, server, accountLimits, nil)

	// Check emergency promotions
	emergencyPromotions := 0
	for _, change := range changes {
		if change.reason == "emergency promotion: zero active monitors" {
			emergencyPromotions++

			// Verify only healthy, globally active monitors were promoted
			for _, em := range evaluatedMonitors {
				if em.monitor.ID == change.monitorID {
					if em.monitor.GlobalStatus != ntpdb.MonitorsStatusActive {
						t.Errorf("Emergency promoted non-active monitor %d", em.monitor.ID)
					}
					if em.monitor.HasMetrics && !em.monitor.IsHealthy {
						t.Errorf("Emergency promoted unhealthy monitor %d", em.monitor.ID)
					}
				}
			}
		}
	}

	// Should have at least one emergency promotion
	if emergencyPromotions == 0 {
		t.Error("Expected emergency promotions when zero active monitors")
	}

	// Debug: let's see which monitors were promoted
	for _, change := range changes {
		t.Logf("Change: monitor %d from %s to %s, reason: %s",
			change.monitorID, change.fromStatus, change.toStatus, change.reason)
	}

	// With change limits and emergency override, multiple promotions are possible
	// Monitor 2 is unhealthy, Monitor 3 is not globally active
	// So only monitor 1 should be promoted, but change limits might allow more
	if emergencyPromotions < 1 {
		t.Errorf("Expected at least 1 emergency promotion, got %d", emergencyPromotions)
	}
}

// TestPromotionConstraintChecking tests that constraints are checked against
// the target state for promotions
func TestPromotionConstraintChecking(t *testing.T) {
	sl := &Selector{
		log: testLogger(),
	}

	// Monitor has limit violation in testing but not in active
	monitor := &monitorCandidate{
		ID:           1,
		AccountID:    ptr(uint32(1)),
		IP:           "10.0.0.1",
		GlobalStatus: ntpdb.MonitorsStatusActive,
		ServerStatus: ntpdb.ServerScoresStatusTesting,
		IsHealthy:    true,
		HasMetrics:   true,
	}

	server := &serverInfo{
		ID:        1,
		AccountID: ptr(uint32(2)),
		IP:        "192.168.1.1",
	}

	// Testing limit reached (2), but active limit not reached (1 < 2)
	// Total is 1 + 2 = 3, which is at the max total limit (MaxPerServer + 1 = 3)
	// But since this monitor is already in testing, it's already counted in TestingCount
	accountLimits := map[uint32]*accountLimit{
		1: {AccountID: 1, MaxPerServer: 2, ActiveCount: 1, TestingCount: 2},
	}

	// Create some dummy assigned monitors for the constraint check
	assignedMonitors := []ntpdb.GetMonitorPriorityRow{}

	// Should be able to promote to active
	canPromote := sl.canPromoteToActive(monitor, server, accountLimits, assignedMonitors, false)
	if !canPromote {
		// Add debug output
		violation := sl.checkConstraints(monitor, server, accountLimits, ntpdb.ServerScoresStatusActive, assignedMonitors)
		t.Errorf("Should be able to promote to active when active limit not reached. Violation: %+v", violation)
	}

	// For more accurate testing, let's also verify with empty account limits
	// to isolate the constraint checking
	emptyLimits := map[uint32]*accountLimit{
		1: {AccountID: 1, MaxPerServer: 2, ActiveCount: 0, TestingCount: 0},
	}
	canPromoteEmpty := sl.canPromoteToActive(monitor, server, emptyLimits, assignedMonitors, false)
	if !canPromoteEmpty {
		t.Error("Should be able to promote to active with no existing monitors")
	}
}

// Helper functions
func ptr(v uint32) *uint32 {
	return &v
}

func testLogger() *slog.Logger {
	return slog.Default()
}
